{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade Prediction Analysis - Universidad AutÃ³noma de Chile\n",
    "\n",
    "**Objective:** Determine feasibility of predicting student grades using LMS engagement data from Canvas.\n",
    "\n",
    "**Data Sources:**\n",
    "- Student summaries (aggregated engagement metrics)\n",
    "- Enrollments (activity time, grades)\n",
    "- Submissions (assignment grades, timing)\n",
    "- Course activity (daily patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "DATA_DIR = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all extracted data\n",
    "with open(f'{DATA_DIR}/student_summaries.json') as f:\n",
    "    student_summaries = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/enrollments.json') as f:\n",
    "    enrollments = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/submissions.json') as f:\n",
    "    submissions = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/courses_raw.json') as f:\n",
    "    courses = json.load(f)\n",
    "\n",
    "with open(f'{DATA_DIR}/assignments.json') as f:\n",
    "    assignments = json.load(f)\n",
    "\n",
    "print(f\"Student summaries: {len(student_summaries)}\")\n",
    "print(f\"Enrollments: {len(enrollments)}\")\n",
    "print(f\"Submissions: {len(submissions)}\")\n",
    "print(f\"Courses: {len(courses)}\")\n",
    "print(f\"Assignments: {len(assignments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "df_summaries = pd.DataFrame(student_summaries)\n",
    "df_enrollments = pd.DataFrame(enrollments)\n",
    "df_submissions = pd.DataFrame(submissions)\n",
    "df_courses = pd.DataFrame(courses)\n",
    "df_assignments = pd.DataFrame(assignments)\n",
    "\n",
    "print(\"\\nStudent Summaries columns:\")\n",
    "print(df_summaries.columns.tolist())\n",
    "\n",
    "print(\"\\nEnrollments columns:\")\n",
    "print(df_enrollments.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Student Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine student summary structure\n",
    "print(\"Sample student summary:\")\n",
    "print(json.dumps(student_summaries[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tardiness breakdown into separate columns\n",
    "if 'tardiness_breakdown' in df_summaries.columns:\n",
    "    tardiness_df = pd.json_normalize(df_summaries['tardiness_breakdown'])\n",
    "    tardiness_df.columns = [f'tardiness_{c}' for c in tardiness_df.columns]\n",
    "    df_summaries = pd.concat([df_summaries.drop('tardiness_breakdown', axis=1), tardiness_df], axis=1)\n",
    "\n",
    "print(\"\\nStudent Summaries shape:\", df_summaries.shape)\n",
    "df_summaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nEngagement Statistics:\")\n",
    "df_summaries[['page_views', 'participations', 'page_views_level', 'participations_level']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Enrollments & Grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check enrollment structure\n",
    "print(\"Sample enrollment:\")\n",
    "print(json.dumps(enrollments[0], indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract grades from enrollments\n",
    "if 'grades' in df_enrollments.columns:\n",
    "    grades_df = pd.json_normalize(df_enrollments['grades'])\n",
    "    grades_df.columns = [f'grade_{c}' for c in grades_df.columns]\n",
    "    df_enrollments = pd.concat([df_enrollments.drop('grades', axis=1), grades_df], axis=1)\n",
    "\n",
    "# Extract user info\n",
    "if 'user' in df_enrollments.columns:\n",
    "    user_df = pd.json_normalize(df_enrollments['user'])\n",
    "    user_df.columns = [f'user_{c}' for c in user_df.columns]\n",
    "    df_enrollments = pd.concat([df_enrollments.drop('user', axis=1), user_df], axis=1)\n",
    "\n",
    "print(\"\\nEnrollments shape:\", df_enrollments.shape)\n",
    "print(\"\\nGrade columns:\")\n",
    "print([c for c in df_enrollments.columns if 'grade' in c.lower() or 'score' in c.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check grade distribution\n",
    "grade_cols = [c for c in df_enrollments.columns if 'current_score' in c or 'final_score' in c]\n",
    "if grade_cols:\n",
    "    print(\"\\nGrade Statistics:\")\n",
    "    print(df_enrollments[grade_cols].describe())\n",
    "    \n",
    "    # Plot grade distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for col in grade_cols:\n",
    "        if df_enrollments[col].notna().sum() > 0:\n",
    "            df_enrollments[col].dropna().hist(bins=20, alpha=0.5, label=col, ax=ax)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Grade Distribution')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common keys\n",
    "print(\"Student Summaries ID column:\", 'id' in df_summaries.columns)\n",
    "print(\"Enrollments user_id column:\", 'user_id' in df_enrollments.columns)\n",
    "\n",
    "# Rename for consistency\n",
    "df_summaries_renamed = df_summaries.rename(columns={'id': 'user_id'})\n",
    "\n",
    "# Merge summaries with enrollments\n",
    "df_merged = df_summaries_renamed.merge(\n",
    "    df_enrollments[['user_id', 'course_id', 'total_activity_time', 'last_activity_at'] + \n",
    "                   [c for c in df_enrollments.columns if 'grade_' in c or 'score' in c.lower()]],\n",
    "    on=['user_id', 'course_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged dataset: {len(df_merged)} student-course records\")\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available features\n",
    "print(\"\\nAvailable features:\")\n",
    "for col in df_merged.columns:\n",
    "    non_null = df_merged[col].notna().sum()\n",
    "    print(f\"  {col}: {non_null}/{len(df_merged)} non-null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate submission-based features per student-course\n",
    "df_sub = df_submissions.copy()\n",
    "\n",
    "# Submission statistics per student per course\n",
    "sub_stats = df_sub.groupby(['user_id', 'course_id']).agg({\n",
    "    'id': 'count',\n",
    "    'score': ['mean', 'std', 'max', 'min'],\n",
    "    'late': 'sum',\n",
    "    'missing': 'sum',\n",
    "    'excused': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "sub_stats.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in sub_stats.columns]\n",
    "sub_stats.columns = ['user_id', 'course_id', 'submission_count', 'score_mean', 'score_std', \n",
    "                     'score_max', 'score_min', 'late_count', 'missing_count', 'excused_count']\n",
    "\n",
    "print(f\"Submission stats: {len(sub_stats)} student-course pairs\")\n",
    "sub_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge submission stats with main dataset\n",
    "df_analysis = df_merged.merge(sub_stats, on=['user_id', 'course_id'], how='left')\n",
    "\n",
    "# Calculate derived features\n",
    "df_analysis['late_pct'] = df_analysis['late_count'] / df_analysis['submission_count'].replace(0, np.nan)\n",
    "df_analysis['missing_pct'] = df_analysis['missing_count'] / df_analysis['submission_count'].replace(0, np.nan)\n",
    "\n",
    "# Engagement score (composite)\n",
    "df_analysis['engagement_score'] = (\n",
    "    df_analysis['page_views_level'] * 0.3 +\n",
    "    df_analysis['participations_level'] * 0.3 +\n",
    "    (1 - df_analysis['late_pct'].fillna(0)) * 0.2 +\n",
    "    (1 - df_analysis['missing_pct'].fillna(0)) * 0.2\n",
    ") * 25  # Scale to 0-100\n",
    "\n",
    "print(f\"\\nAnalysis dataset: {len(df_analysis)} records with {len(df_analysis.columns)} features\")\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features for correlation\n",
    "feature_cols = [\n",
    "    'page_views', 'page_views_level', 'participations', 'participations_level',\n",
    "    'tardiness_on_time', 'tardiness_late', 'tardiness_missing',\n",
    "    'total_activity_time', 'submission_count', 'score_mean',\n",
    "    'late_count', 'missing_count', 'late_pct', 'missing_pct', 'engagement_score'\n",
    "]\n",
    "\n",
    "# Find the target variable (current score or final score)\n",
    "target_candidates = [c for c in df_analysis.columns if 'current_score' in c.lower() or 'final_score' in c.lower()]\n",
    "if target_candidates:\n",
    "    target_col = target_candidates[0]\n",
    "    print(f\"Target variable: {target_col}\")\n",
    "else:\n",
    "    target_col = 'score_mean'\n",
    "    print(f\"Using submission score mean as target: {target_col}\")\n",
    "\n",
    "# Available features\n",
    "available_features = [c for c in feature_cols if c in df_analysis.columns and c != target_col]\n",
    "print(f\"\\nAvailable features: {available_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "correlations = df_analysis[available_features + [target_col]].corr()[target_col].drop(target_col).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nCorrelation with {target_col}:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in correlations.values]\n",
    "correlations.plot(kind='barh', ax=ax, color=colors)\n",
    "ax.set_xlabel('Correlation Coefficient')\n",
    "ax.set_title(f'Feature Correlation with {target_col}')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "corr_matrix = df_analysis[available_features + [target_col]].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn', center=0, fmt='.2f', ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Prepare data for modeling\n",
    "model_features = [c for c in available_features if c != 'score_mean']  # Exclude direct score features\n",
    "\n",
    "# Create clean dataset\n",
    "df_model = df_analysis[model_features + [target_col]].dropna()\n",
    "print(f\"Model dataset: {len(df_model)} complete records\")\n",
    "\n",
    "X = df_model[model_features]\n",
    "y = df_model[target_col]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for linear models\n",
    "    if 'Regression' in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  R2: {r2:.3f}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R2']\n",
    "colors = sns.color_palette('husl', len(df_results))\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    bars = ax.bar(df_results['Model'], df_results[metric], color=colors)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Model Comparison: {metric}')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, df_results[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': model_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Random Forest):\")\n",
    "print(feature_importance)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='Importance', y='Feature', ax=ax, palette='viridis')\n",
    "ax.set_title('Feature Importance for Grade Prediction')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual plot for best model\n",
    "best_model_name = df_results.loc[df_results['R2'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "if 'Regression' in best_model_name:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(y_test, y_pred_best, alpha=0.5)\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual Score')\n",
    "ax.set_ylabel('Predicted Score')\n",
    "ax.set_title(f'Predicted vs Actual Grades ({best_model_name})')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for best models\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name in ['Random Forest', 'Gradient Boosting']:\n",
    "    model = models[name]\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  R2 scores: {scores}\")\n",
    "    print(f\"  Mean R2: {scores.mean():.3f} (+/- {scores.std()*2:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"GRADE PREDICTION FEASIBILITY ANALYSIS - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. DATA OVERVIEW:\")\n",
    "print(f\"   - Total student-course records: {len(df_analysis)}\")\n",
    "print(f\"   - Complete records for modeling: {len(df_model)}\")\n",
    "print(f\"   - Courses analyzed: {df_analysis['course_id'].nunique()}\")\n",
    "print(f\"   - Unique students: {df_analysis['user_id'].nunique()}\")\n",
    "\n",
    "print(f\"\\n2. BEST PREDICTIVE MODEL:\")\n",
    "best_idx = df_results['R2'].idxmax()\n",
    "print(f\"   - Model: {df_results.loc[best_idx, 'Model']}\")\n",
    "print(f\"   - R2 Score: {df_results.loc[best_idx, 'R2']:.3f}\")\n",
    "print(f\"   - RMSE: {df_results.loc[best_idx, 'RMSE']:.2f} points\")\n",
    "print(f\"   - MAE: {df_results.loc[best_idx, 'MAE']:.2f} points\")\n",
    "\n",
    "print(f\"\\n3. TOP PREDICTIVE FEATURES:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.3f}\")\n",
    "\n",
    "print(f\"\\n4. STRONGEST CORRELATIONS WITH GRADES:\")\n",
    "top_corr = correlations.head(5)\n",
    "for feat, corr in top_corr.items():\n",
    "    print(f\"   - {feat}: r={corr:.3f}\")\n",
    "\n",
    "print(f\"\\n5. CONCLUSION:\")\n",
    "r2_best = df_results.loc[best_idx, 'R2']\n",
    "if r2_best > 0.5:\n",
    "    print(f\"   FEASIBLE: LMS engagement data can explain {r2_best*100:.1f}% of grade variance.\")\n",
    "    print(\"   The model shows strong predictive power for early warning systems.\")\n",
    "elif r2_best > 0.3:\n",
    "    print(f\"   MODERATELY FEASIBLE: LMS data explains {r2_best*100:.1f}% of grade variance.\")\n",
    "    print(\"   Useful for identifying at-risk students but not for precise predictions.\")\n",
    "else:\n",
    "    print(f\"   LIMITED FEASIBILITY: LMS data only explains {r2_best*100:.1f}% of grade variance.\")\n",
    "    print(\"   Other factors (prior knowledge, external study, etc.) may be more important.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "results_summary = {\n",
    "    'total_records': len(df_analysis),\n",
    "    'model_records': len(df_model),\n",
    "    'courses': int(df_analysis['course_id'].nunique()),\n",
    "    'students': int(df_analysis['user_id'].nunique()),\n",
    "    'best_model': df_results.loc[best_idx, 'Model'],\n",
    "    'best_r2': float(df_results.loc[best_idx, 'R2']),\n",
    "    'best_rmse': float(df_results.loc[best_idx, 'RMSE']),\n",
    "    'best_mae': float(df_results.loc[best_idx, 'MAE']),\n",
    "    'top_features': feature_importance.head(5).to_dict('records'),\n",
    "    'model_comparison': df_results.to_dict('records')\n",
    "}\n",
    "\n",
    "with open(f'{DATA_DIR}/grade_prediction_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to {DATA_DIR}/grade_prediction_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
